{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaplan Meier development\n",
    "## Original community algorithm\n",
    "Make sure to have R installed with the required dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import import_module\n",
    "_km = import_module(\"v6-kaplan-meier-py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vantage6.algorithm.tools.mock_client import MockAlgorithmClient\n",
    "# Initialize the mock server. The datasets simulate the local datasets from\n",
    "# the node. In this case we have two parties having two different datasets:\n",
    "# a.csv and b.csv. The module name needs to be the name of your algorithm\n",
    "# package. This is the name you specified in `setup.py`, in our case that\n",
    "# would be v6-correlation-matrix-py.\n",
    "dataset_1 = {\"database\": \"/Users/frankmartin/Projects/Euracan/small_df.parquet\", \"db_type\": \"parquet\"}\n",
    "dataset_2 = {\"database\": \"/Users/frankmartin/Projects/Euracan/small_df.parquet\", \"db_type\": \"parquet\"}\n",
    "dataset_3 = {\"database\": \"/Users/frankmartin/Projects/Euracan/small_df.parquet\", \"db_type\": \"parquet\"}\n",
    "org_ids = ids = [0, 1, 2]\n",
    "\n",
    "client = MockAlgorithmClient(\n",
    "    datasets=[[dataset_1], [dataset_2], [dataset_3]],\n",
    "    organization_ids=org_ids,\n",
    "    module=\"v6-kaplan-meier-py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import base64\n",
    "STRING_ENCODING = \"utf-8\"\n",
    "ENV_VAR_EQUALS_REPLACEMENT = \"=\"\n",
    "def _encode(string: str):\n",
    "    return (\n",
    "        base64.b32encode(string.encode(STRING_ENCODING))\n",
    "        .decode(STRING_ENCODING)\n",
    "        .replace(\"=\", ENV_VAR_EQUALS_REPLACEMENT)\n",
    "    )\n",
    "os.environ[\"KAPLAN_MEIER_TYPE_NOISE\"] = _encode(\"NONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations = client.organization.list()\n",
    "org_ids = ids = [organization[\"id\"] for organization in organizations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Note that we only supply the task to a single organization as we only want to execute\n",
    "# the central part of the algorithm once. The master task takes care of the\n",
    "# distribution to the other parties.\n",
    "average_task = client.task.create(\n",
    "    input_={\n",
    "        \"method\": \"central\",\n",
    "        \"kwargs\": {\n",
    "            \"time_column_name\": \"SURVIVAL_DAYS\",\n",
    "            \"censor_column_name\": \"CENSOR\",\n",
    "        },\n",
    "    },\n",
    "    organizations=[org_ids[0]],\n",
    ")\n",
    "\n",
    "results = client.result.get(average_task.get(\"id\"))\n",
    "df_events_clean = pd.read_json(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot the Kaplan-Meier curve for clean data\n",
    "ax1.plot(\n",
    "    df_events_clean[\"SURVIVAL_DAYS\"], df_events_clean[\"survival_cdf\"], label=\"Clean Data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarcoma registry extension\n",
    "- [ ] Stratify on categorical variables\n",
    "- [ ] make it multi-cohort / session based data input\n",
    "\n",
    "\n",
    "### Stratify on categorical variables\n",
    "The algorithm needs to be extended in order to support stratification on categorical variables. This simply means splitting the dataset on the categorical variable and computing the Kaplan Meier curve for each group.\n",
    "\n",
    "* There are two partial functions: `get_unique_event_times` and `get_km_event_table`  see [here](https://github.com/vantage6/v6-kaplan-meier-py/blob/main/v6-kaplan-meier-py/partial.py). You should split the dataframe before reporting the results (and the results should be reported per strata).\n",
    "* The the unique event times should be combined per strata somewhere [here](https://github.com/vantage6/v6-kaplan-meier-py/blob/e7ea6466f548902945c3f757f5ed036790ae2ea9/v6-kaplan-meier-py/central.py#L68-L78)\n",
    "* And finally you need to do the same thing for the event table [here](https://github.com/vantage6/v6-kaplan-meier-py/blob/e7ea6466f548902945c3f757f5ed036790ae2ea9/v6-kaplan-meier-py/central.py#L80-L96)\n",
    "\n",
    "### Multi-cohort / session\n",
    "The algorithm currently works with only a single dataset supplied by the infrastructure. \n",
    "\n",
    "* In the current setup of the sarcoma registry we do not use this mechanism. Instead of using the node dataset we created copies of the output of the queries. These are stored in a directory where succesive algorithms can find it. This implies that we cannot use the default `@data` decorator to inject the algorithm with the data (see for example here: https://github.com/vantage6/v6-kaplan-meier-py/blob/e7ea6466f548902945c3f757f5ed036790ae2ea9/v6-kaplan-meier-py/partial.py#L20). \n",
    "* Another addition is to be able to compute the kaplan meier curve for multiple datasets at the same time (this saves us from having to run the algorithm multiple times on the infra making it very slow).\n",
    "\n",
    "So what needs to be done:\n",
    "\n",
    "* The `get_unique_event_times` and `get_km_event_table` first need to be executable without the `@data` decorator. This means that the data should be passed as an argument. In the summary algorithm I submitted a patch that does this, see the `summary_per_data_station` and `_summary_per_data_station` functions [here](https://github.com/vantage6/v6-summary-py/blob/bbdb1626bcfdf82659cd1ba82d6715e7060ad2fd/v6-summary-py/partial_summary.py#L24C5-L53). This way we can wrap our own decorator around it that does the same thing as the `@data` decorator but for multiple datasets and with our session setup. This new decorator is defined as:\n",
    "    ```python\n",
    "    from functools import wraps\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    from vantage6.algorithm.tools.util import info\n",
    "    from vantage6.algorithm.tools.decorators import _get_user_database_labels\n",
    "\n",
    "\n",
    "    def new_data_decorator(func: callable, *args, **kwargs) -> callable:\n",
    "        @wraps(func)\n",
    "        def decorator(*args, mock_data: list[pd.DataFrame] = None, **kwargs) -> callable:\n",
    "\n",
    "            if mock_data:\n",
    "                data_frames = mock_data\n",
    "                cohort_names = [f\"cohort_{i}\" for i in range(len(mock_data))]\n",
    "            else:\n",
    "                cohort_names = _get_user_database_labels()\n",
    "                data_frames = []\n",
    "                for cohort_name in cohort_names:\n",
    "                    info(f\"Loading data for cohort {cohort_name}\")\n",
    "\n",
    "                    df = pd.read_parquet(f\"/mnt/data/{cohort_name}.parquet\")\n",
    "                    data_frames.append(df)\n",
    "\n",
    "            args = (data_frames, cohort_names, *args)\n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "        decorator.wrapped_in_data_decorator = True\n",
    "        return decorator\n",
    "    ```\n",
    "See how ive done this in [here](https://github.com/Franky-Codes-BV/v6-euracan-sarcoma-algorithms/blob/feature/add-summary-stats-per-var-and-cohort/v6-algorithm-wrapper-py/v6-algorithm-wrapper-py/__init__.py)\n",
    "Note that I was able to reuse the partial functions from the summary community algorithm, but I had to rewrite the central function as the aggregation steps are different (because of the nested cohort results). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blueberry-algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
